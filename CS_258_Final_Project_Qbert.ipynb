{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning with Q*bert\n",
        "\n",
        "CS 258\n",
        "\n",
        "Hao Feng\n",
        "\n",
        "861090340"
      ],
      "metadata": {
        "id": "wHshZEsswsfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "GK2xGyY8w_d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for compatibility\n",
        "# !apt update && apt install cuda-11-8"
      ],
      "metadata": {
        "id": "Nus_0pdNowmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdMbsgDWFSas",
        "outputId": "6c400dff-a2b4-4da8-b8b4-81dd00a8db05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device at: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "if torch.cuda.is_available():\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('Found device at: {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apc921xLKLNF",
        "outputId": "78c35dc5-cab7-405a-a8bd-fd8498a7d3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.4.0)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.4)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiZuKVZQL7nM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from gymnasium.wrappers.record_video import RecordVideo\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X2F_8LNN3Ky",
        "outputId": "8279a90f-ff7e-412b-faf6-8c3b32a268c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_writer = SummaryWriter(log_dir='tensorboard/qbert')"
      ],
      "metadata": {
        "id": "7jALVq1Pz_2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q Network"
      ],
      "metadata": {
        "id": "jI0PBg1lxCEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snteI1BxRg2L"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self,state_dim,action_dim,hidden_dims,kernel_size,stride):\n",
        "        super(QNetwork, self).__init__()\n",
        "        layers = []\n",
        "        input_channels = state_dim[-1]\n",
        "        h,w = state_dim[0],state_dim[1]\n",
        "        for dim in hidden_dims:\n",
        "            layers.append(nn.Conv2d(input_channels,dim,kernel_size=kernel_size,stride=stride))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_channels = dim\n",
        "            h = (h-kernel_size)//stride+1\n",
        "            w = (w-kernel_size)//stride+1\n",
        "\n",
        "        layers.append(nn.Flatten())\n",
        "        flattened = input_channels*h*w\n",
        "        layers.append(nn.Linear(flattened, action_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # print(state.shape)\n",
        "        return self.model(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer"
      ],
      "metadata": {
        "id": "MtDPT37AxJTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self,capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self,state,action,reward,next_state,done):\n",
        "        self.buffer.append((state,action,reward,next_state,done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state,action,reward,next_state,done = zip(*random.sample(self.buffer,batch_size))\n",
        "        return (np.array(state),np.array(action),np.array(reward),np.array(next_state),np.array(done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "6hUTDgoqxLDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "luhiWF0ixMo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z0jqmx_RlZd"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self,config):\n",
        "        self.device = config['device']\n",
        "        self.state_dim = config['state_dim']\n",
        "        self.action_dim = config['action_dim']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.kernel_size = config['kernel_size']\n",
        "        self.stride = config['stride']\n",
        "\n",
        "        self.lr = config['lr']\n",
        "        self.gamma = config['gamma']\n",
        "        self.tau = config['tau']\n",
        "        self.replay_buffer = ReplayBuffer(config['buffer_capacity'])\n",
        "        self.batch_size = config['batch_size']\n",
        "\n",
        "        self.eps_max = config['eps_max']\n",
        "        self.eps_min = config['eps_min']\n",
        "        self.eps_decay = config['eps_decay']\n",
        "        self.steps_done = 0\n",
        "\n",
        "        self.q_network = QNetwork(self.state_dim,self.action_dim,self.hidden_dims,self.kernel_size,self.stride).to(self.device)\n",
        "        self.q_target = QNetwork(self.state_dim,self.action_dim,self.hidden_dims,self.kernel_size,self.stride).to(self.device)\n",
        "        self.q_target.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(),lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "        self.epsilon = max(self.eps_max*(self.eps_decay-self.steps_done)/self.eps_decay,self.eps_min)\n",
        "\n",
        "        if random.random() > self.epsilon:\n",
        "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                return self.q_network(state).argmax(dim=1).item()\n",
        "        else:\n",
        "            # random action otherwise\n",
        "            return random.randrange(self.q_network.model[-1].out_features)\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        state,action,reward,next_state,done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        action = torch.LongTensor(action).to(self.device).unsqueeze(-1)\n",
        "        reward = torch.FloatTensor(reward).to(self.device)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        done = torch.FloatTensor(done).to(self.device)\n",
        "\n",
        "        current_q_values = self.q_network(state).gather(1,action).squeeze(-1)\n",
        "\n",
        "        # target q values determined by target q values for next states\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.q_target(next_state).max(1)[0]\n",
        "            target_q_values = reward+(1-done)*self.gamma*max_next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # soft update\n",
        "        for target_param,param in zip(self.q_target.parameters(),self.q_network.parameters()):\n",
        "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
        "\n",
        "    def store_transition(self,state,action,reward,next_state,done):\n",
        "        self.replay_buffer.push(state,action,reward,next_state,done)\n",
        "\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self,config):\n",
        "        self.device = config['device']\n",
        "        self.state_dim = config['state_dim']\n",
        "        self.action_dim = config['action_dim']\n",
        "        self.hidden_dims = config['hidden_dims']\n",
        "        self.kernel_size = config['kernel_size']\n",
        "        self.stride = config['stride']\n",
        "\n",
        "        self.lr = config['lr']\n",
        "        self.gamma = config['gamma']\n",
        "        self.tau = config['tau']\n",
        "        self.replay_buffer = ReplayBuffer(config['buffer_capacity'])\n",
        "        self.batch_size = config['batch_size']\n",
        "\n",
        "        self.eps_max = config['eps_max']\n",
        "        self.eps_min = config['eps_min']\n",
        "        self.eps_decay = config['eps_decay']\n",
        "        self.steps_done = 0\n",
        "\n",
        "        self.q_network = QNetwork(self.state_dim,self.action_dim,self.hidden_dims,self.kernel_size,self.stride).to(self.device)\n",
        "        self.q_target = QNetwork(self.state_dim,self.action_dim,self.hidden_dims,self.kernel_size,self.stride).to(self.device)\n",
        "        self.q_target.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(),lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "        self.epsilon = max(self.eps_max*(self.eps_decay-self.steps_done)/self.eps_decay,self.eps_min)\n",
        "\n",
        "        if random.random() > self.epsilon:\n",
        "            state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                return self.q_network(state).argmax(dim=1).item()\n",
        "        else:\n",
        "            # random action otherwise\n",
        "            return random.randrange(self.q_network.model[-1].out_features)\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        state,action,reward,next_state,done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        action = torch.LongTensor(action).to(self.device).unsqueeze(-1)\n",
        "        reward = torch.FloatTensor(reward).to(self.device)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        done = torch.FloatTensor(done).to(self.device)\n",
        "\n",
        "        current_q_values = self.q_network(state).gather(1,action).squeeze(-1)\n",
        "\n",
        "        # online network for action select, target net for evaluation\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_network(next_state).argmax(1,keepdim=True)\n",
        "            max_next_q_values = self.q_target(next_state).gather(1,next_actions).squeeze(-1)\n",
        "            target_q_values = reward+(1-done)*self.gamma*max_next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # soft update\n",
        "        for target_param,param in zip(self.q_target.parameters(),self.q_network.parameters()):\n",
        "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
        "\n",
        "    def store_transition(self,state,action,reward,next_state,done):\n",
        "        self.replay_buffer.push(state,action,reward,next_state,done)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Run"
      ],
      "metadata": {
        "id": "HaO39gbCwggk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for compatibility\n",
        "# torch.backends.cudnn.enabled = False"
      ],
      "metadata": {
        "id": "NN8E2u7gnQfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00nI8v1XOex4"
      },
      "outputs": [],
      "source": [
        "env = gym.make('ALE/Qbert-v5',render_mode='rgb_array')\n",
        "env = RecordVideo(env,'./videos_dqn',episode_trigger = lambda x: x%20==0)\n",
        "state_dim = env.observation_space.shape\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "config = {\n",
        "    'state_dim': state_dim,\n",
        "    'action_dim': action_dim,\n",
        "    'hidden_dims': [16, 32],\n",
        "    'lr': 1e-4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.005,\n",
        "    'buffer_capacity': 40000,\n",
        "    'batch_size': 32,\n",
        "    'eps_max': 1.0,\n",
        "    'eps_min': 0.01,\n",
        "    'eps_decay': 10000,\n",
        "    'kernel_size': 8,\n",
        "    'stride': 4,\n",
        "    'device': device,\n",
        "}\n",
        "\n",
        "agent = DQNAgent(config)\n",
        "\n",
        "num_episodes = 1000\n",
        "max_timesteps = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state,_ = env.reset()\n",
        "    state = state.transpose(2,0,1)\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        action = agent.select_action(state)\n",
        "        next_state,reward,done,_,_ = env.step(action)\n",
        "        next_state = next_state.transpose(2,0,1)\n",
        "        agent.store_transition(state,action,reward,next_state,done)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        agent.update()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
        "    train_writer.add_scalar('Performance/episodic_return_dqn', episode_reward, episode)\n",
        "\n",
        "env.close()\n",
        "train_writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double DQN Run"
      ],
      "metadata": {
        "id": "3ps8tJPlwmq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for compatibility\n",
        "# torch.backends.cudnn.enabled = False"
      ],
      "metadata": {
        "id": "gTxZys13oIHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Qbert-v5',render_mode='rgb_array')\n",
        "env = RecordVideo(env,'./videos_doubledqn',episode_trigger = lambda x: x%20==0)\n",
        "state_dim = env.observation_space.shape\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "config = {\n",
        "    'state_dim': state_dim,\n",
        "    'action_dim': action_dim,\n",
        "    'hidden_dims': [16, 32],\n",
        "    'lr': 1e-4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.005,\n",
        "    'buffer_capacity': 40000,\n",
        "    'batch_size': 32,\n",
        "    'eps_max': 1.0,\n",
        "    'eps_min': 0.01,\n",
        "    'eps_decay': 10000,\n",
        "    'kernel_size': 8,\n",
        "    'stride': 4,\n",
        "    'device': device,\n",
        "}\n",
        "\n",
        "agent = DoubleDQNAgent(config)\n",
        "\n",
        "num_episodes = 1000\n",
        "max_timesteps = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state,_ = env.reset()\n",
        "    state = state.transpose(2,0,1)\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        action = agent.select_action(state)\n",
        "        next_state,reward,done,_,_ = env.step(action)\n",
        "        next_state = next_state.transpose(2,0,1)\n",
        "        agent.store_transition(state,action,reward,next_state,done)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        agent.update()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
        "    train_writer.add_scalar('Performance/episodic_return_doubledqn', episode_reward, episode)\n",
        "\n",
        "env.close()\n",
        "train_writer.close()"
      ],
      "metadata": {
        "id": "52X_XoraxXRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/Load Model"
      ],
      "metadata": {
        "id": "D9FaJWGQw4z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.q_network.state_dict(),'doubledqn_q_network_'+str(episode)+'.pt')\n",
        "torch.save(agent.q_target.state_dict(),'doubledqn_q_target_'+str(episode)+'.pt')\n",
        "\n",
        "import pickle\n",
        "with open('doubledqn_'+str(episode)+'.pkl','wb') as f:\n",
        "    pickle.dump(agent.replay_buffer,f)"
      ],
      "metadata": {
        "id": "y6JGKeJ37-Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.q_network.load_state_dict(torch.load('doubledqn_q_network_394.pt'))\n",
        "agent.q_target.load_state_dict(torch.load('doubledqn_q_target_394.pt'))\n",
        "\n",
        "import pickle\n",
        "with open('doubledqn_394.pkl','rb') as f:\n",
        "    agent.replay_buffer = pickle.load(f)"
      ],
      "metadata": {
        "id": "GKS52w5Ex4vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "JgEqhnNewql7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEkrSLr8Frq7"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir='tensorboard/qbert'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}